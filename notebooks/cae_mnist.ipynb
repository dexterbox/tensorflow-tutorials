{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " Convolutional AutoEncoder\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline \n",
    "\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "\n",
    "trainimgs   = mnist.train.images\n",
    "trainlabels = mnist.train.labels\n",
    "testimgs    = mnist.test.images\n",
    "testlabels  = mnist.test.labels\n",
    "\n",
    "ntrain      = trainimgs.shape[0]\n",
    "ntest       = testimgs.shape[0]\n",
    "dim         = trainimgs.shape[1]\n",
    "nout        = trainlabels.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define convolutional auto-encoder (CAE)\n",
    "\n",
    "n1 = 10\n",
    "n2 = 20\n",
    "n3 = 30\n",
    "\n",
    "weights = {\n",
    "    'ce1': tf.Variable(tf.random_normal([3, 3, 1, n1], stddev=0.1)),\n",
    "    'ce2': tf.Variable(tf.random_normal([3, 3, n1, n2], stddev=0.1)),\n",
    "    'ce3': tf.Variable(tf.random_normal([3, 3, n2, n3], stddev=0.1)),\n",
    "    'cd3': tf.Variable(tf.random_normal([3, 3, n2, n3], stddev=0.1)),\n",
    "    'cd2': tf.Variable(tf.random_normal([3, 3, n1, n2], stddev=0.1)),\n",
    "    'cd1': tf.Variable(tf.random_normal([3, 3, 1, n1], stddev=0.1))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'be1': tf.Variable(tf.random_normal([n1], stddev=0.1)),\n",
    "    'be2': tf.Variable(tf.random_normal([n2], stddev=0.1)),\n",
    "    'be3': tf.Variable(tf.random_normal([n3], stddev=0.1)),\n",
    "    'bd3': tf.Variable(tf.random_normal([n2], stddev=0.1)),\n",
    "    'bd2': tf.Variable(tf.random_normal([n1], stddev=0.1)),\n",
    "    'bd1': tf.Variable(tf.random_normal([1], stddev=0.1))\n",
    "}\n",
    "def cae(_X, _W, _b):\n",
    "    _input_r = tf.reshape(_X, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    # Encoder\n",
    "    _ce1 = tf.nn.relu(\n",
    "        tf.add(\n",
    "            tf.nn.conv2d(_input_r, _W['ce1'], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            , _b['be1']\n",
    "        ))\n",
    "    _ce2 = tf.nn.relu(\n",
    "        tf.add(\n",
    "            tf.nn.conv2d(_ce1, _W['ce2'], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            , _b['be2']\n",
    "        )) \n",
    "    _ce3 = tf.nn.relu(\n",
    "        tf.add(\n",
    "            tf.nn.conv2d(_ce2, _W['ce3'], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            , _b['be3']\n",
    "        )) \n",
    "    \n",
    "    # Decoder\n",
    "    _cd3 = tf.nn.relu(\n",
    "        tf.add(\n",
    "            tf.nn.conv2d_transpose(_ce3, _W['cd3']\n",
    "                , tf.pack([tf.shape(_X)[0], 7, 7, n2])\n",
    "                , strides=[1, 2, 2, 1], padding='SAME')\n",
    "            , _b['bd3']\n",
    "        )) \n",
    "    _cd2 = tf.nn.relu(\n",
    "        tf.add(\n",
    "            tf.nn.conv2d_transpose(_cd3, _W['cd2'] # <== ERROR!\n",
    "                , tf.pack([tf.shape(_X)[0], 14, 14, n1])\n",
    "                , strides=[1, 2, 2, 1], padding='SAME') \n",
    "            , _b['bd2']\n",
    "        )) \n",
    "    _cd1 = tf.nn.relu(\n",
    "        tf.add(\n",
    "            tf.nn.conv2d_transpose(_cd2, _W['cd1']\n",
    "                , tf.pack([tf.shape(_X)[0], 28, 28, 1])\n",
    "                , strides=[1, 2, 2, 1], padding='SAME')\n",
    "            , _b['bd1']\n",
    "        )) \n",
    "    _out = _cd1\n",
    "    return {'input_r': _input_r\n",
    "            , 'ce1': _ce1, 'ce2': _ce2, 'ce3': _ce3\n",
    "            , 'cd3': _cd3, 'cd2': _cd2, 'cd1': _cd1\n",
    "            , 'layers': (_input_r, _ce1, _ce2, _ce3, _cd3, _cd2, _cd1)\n",
    "            , 'out': _out}\n",
    "\n",
    "# Define functions\n",
    "x = tf.placeholder(tf.float32, [None, dim])\n",
    "pred = cae(x, weights, biases)['out']\n",
    "cost = tf.reduce_sum(tf.square(cae(x, weights, biases)['input_r'] \n",
    "            - cae(x, weights, biases)['out']))\n",
    "learning_rate = 0.001\n",
    "optm = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "init = tf.initialize_all_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Strart training..\n",
      "0 / 10 5075.16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We create a session to use the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# %%\n",
    "mnist = input_data.read_data_sets('data', one_hot=True)\n",
    "mean_img = np.mean(mnist.train.images, axis=0)\n",
    "\n",
    "# Fit all training data\n",
    "batch_size = 100\n",
    "n_epochs   = 10\n",
    "print(\"Strart training..\")\n",
    "for epoch_i in range(n_epochs):\n",
    "    for batch_i in range(mnist.train.num_examples // batch_size):\n",
    "        batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "        train = np.array([img - mean_img for img in batch_xs])\n",
    "        sess.run(optm, feed_dict={x: train})\n",
    "    print(epoch_i, \"/\", n_epochs, sess.run(cost, feed_dict={x: train}))\n",
    "\n",
    "print(\"Training done. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot example reconstructions\n",
    "n_examples = 10\n",
    "test_xs, _ = mnist.test.next_batch(n_examples)\n",
    "test_xs_norm = np.array([img - mean_img for img in test_xs])\n",
    "recon = sess.run(pred, feed_dict={x: test_xs_norm})\n",
    "print(recon.shape)\n",
    "fig, axs = plt.subplots(2, n_examples, figsize=(10, 2))\n",
    "for example_i in range(n_examples):\n",
    "    axs[0][example_i].matshow(\n",
    "        np.reshape(test_xs[example_i, :], (28, 28)), cmap=plt.get_cmap('gray'))\n",
    "    axs[1][example_i].matshow(\n",
    "        np.reshape(\n",
    "            np.reshape(recon[example_i, ...], (784,)) + mean_img,\n",
    "            (28, 28)), cmap=plt.get_cmap('gray'))\n",
    "plt.draw()\n",
    "plt.show()\n",
    "\n",
    "test_xs, _ = mnist.test.next_batch(1)\n",
    "test_xs_norm = np.array([img - mean_img for img in test_xs])\n",
    "recon = sess.run(pred, feed_dict={x: test_xs_norm})\n",
    "layers = sess.run(cae(x, weights, biases)['layers'], feed_dict={x: test_xs_norm})\n",
    "for i in range(len(layers)):\n",
    "    currl = layers[i]\n",
    "    print ((\"layer %d's shape is %s\") % (i, currl.shape,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
